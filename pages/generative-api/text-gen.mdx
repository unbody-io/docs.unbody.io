# Text Generation API

The `.generate.text()` method creates text content based on a prompt or a sequence of messages. It supports configurable options for model selection, token limits, and streaming for real-time generation.

## Usage

### Example 1 - Basic Prompt
Generate text using a simple string prompt:
```typescript copy
unbody.generate.text("Explain AI's impact on healthcare.");
```

### Example 2 - Prompt with Options
Add options to control the output:
```typescript copy
unbody.generate.text("Write a report on climate change.", {
  model: "gpt-4",
  topP: 0.7,
  maxTokens: 1000,
  temperature: 0.7,
});
```

### Example 3 - Message-Based Input
Guide the generation using a series of messages:
```typescript copy
unbody.generate.text(
  [
    { role: "system", content: "You are an AI specialist." },
    { role: "user", content: "What are the key AI trends in 2024?" },
  ],
  {
    model: "gpt-4",
    temperature: 0.7,
    maxTokens: 1000,
  }
);
```

### Example 4 - Streaming Text Generation
Generate text with real-time streaming:
```typescript copy
const stream = await unbody.generate.text(
  "Write a detailed explanation of quantum computing.",
  {
    model: "gpt-4",
    temperature: 0.7,
    stream: true,
  }
);

for await (const chunk of stream) {
  if (chunk.finished) {
    console.log('Stream finished');
    console.log('Final response:', chunk.content);
    break;
  } else {
    console.log(chunk.content);
  }
}
```

## Options
- `model`: AI model to use (e.g., `gpt-4`). 
- `topP`: Diversity control via nucleus sampling (0 to 1).
- `maxTokens`: Maximum length of the response.
- `temperature`: Creativity adjustment (0 to 1).
- `presencePenalty`: Penalize new tokens based on input presence (-2.0 to 2.0).
- `frequencyPenalty`: Penalize tokens based on frequency of occurrence (-2.0 to 2.0).
- `stream`: Enable streaming for real-time content generation (boolean).

